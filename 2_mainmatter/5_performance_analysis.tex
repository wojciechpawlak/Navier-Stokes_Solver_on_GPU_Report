\chapter{Performance Analysis}
\section{General}
For detailed analysis of the kernel performance various conditions should be checked. Test of OpenCL application should be run across different architectures to check its portability and how code behaves on different machines. The kernels should also scale with problem size. Speedups should be measured between version that execute on host and on device. Finally, as the computation is done on floating point number, comparison between single and double precision might yield interesting results.

Synchronization between work-items is possible only within workgroups using barriers and memory fences. It is not possible to synchronize workitems that are in different workgroups as this would hinder data parallelism.

Memory management is explicit, so a programmer moves data from host to global memory of device first using OpenCL constructs. Then it can be moved to local memory. To read the results from the device local memory on the host the process has to be reverted step by step.

Workgroup size for a given algorithm should be selected in such way as to be an even multiple of the width of the hardware scheduling units.

Events can be used to synchronize multiple kernels that are in the queue.
			

\subsection{Timing performance}
OpenCL calls and kernel executions can be timed using either CPU or GPU timers. This section examines the functionality, advantages, and pitfalls of both approaches.\cite{nvidia2009openclbest}

Using CPU Timers
Any CPU timer can be used to measure the elapsed time of an OpenCL call. \cite{nvidia2009openclbest}

When using CPU timers, it is critical to remember that some OpenCL function calls can be non-blocking; that is, they return control back to the calling CPU thread prior to completing their work. All kernel execution enqueue calls are non-blocking; so are all memory transfer enqueue calls with the blocking parameter set to true.Therefore, to accurately measure the elapsed time for a particular call or sequence of OpenCL calls, it is necessary to synchronize the CPU thread with the GPU by calling clFinish() for all command queues immediately before starting and stopping the CPU timer. clFinish()blocks the calling CPU thread until all OpenCL calls previously issued by the thread are completed.\cite{nvidia2009openclbest}

Using OpenCL GPU Timers

Each enqueue call optionally returns an event object that uniquely identifies the enqueued command. The event object of a command can be used to measure its execution time if as detailed in Section 5.9 and illustrated in Listing 2.1. Profiling can be enabled by setting the $CL_QUEUE_PROFILING_ENABLE$ flag in properties argument of either clCreateCommandQueue or clSetCommandQueueProperty.\cite{nvidia2009openclbest}


\section{Computed results}
For computed results the effective bandwidth should be compared with theoretic maximal bandwidth. This way the occupancy of the device can be measured.

Trap of premature optimization should be avoided so the kernel should be always first checked against the sequential code to yield valid results.

Visual Profiler Recommendations cane be followed, but they should be first understood

Bandwidth between host and device should be kept high alll the time.

Data Transfer Between Host and Device

PCIe Bus is used to connect between CPU and GPU 
(8 GBps on the PCI Express Ã—16 Gen2).
Hence, for best overall application performance, it is important to minimize data
transfer between the host and the device, even if that means running kernels on the
GPU that do not demonstrate any speed-up compared with running them on the
host CPU.


Intermediate data structures should be created in device memory, operated on by
the device, and destroyed without ever being mapped by the host or copied to host
memory.\cite{nvidia2009openclbest}

Also, because of the overhead associated with each transfer, batching many small
transfers into one larger transfer performs significantly better than making each
transfer separately.\cite{nvidia2009openclbest}



Coalescing global memory accesses

compute capability 2.x

Branching and Divergence
Avoid different execution paths within the same warp.


built-in math functions, fast optimized library

Synchronization on Global and Local barriers

Synchronization between kernels in the queue. All the kernels are waiting for another to finish to work on nits results.
One device - one queue.

CPU code in VIsual studio 2010 basic release build optimizations on both CPU execution and CPU control parts of GPU execution

Asynchronous reading of memory $CL_FALSE$ instead of $CL_TRUE$

allocating memory without copying from host pointer

No print options in the loop

Workgroup size

Size of the grid

%Essentially you should strive for doing all computations on the device only (unless you aim for very large distributed calculations, which requires some more effort). That is, transfer all necessary data to device and let it stay there for as long as possible while doing calculations and then port the results back to the host in the end.
%
%
%This speedup is on your laptop? For a high-end CPU and GPU I would expect,
%say, 5 times speedup in a naive transfer due to the difference in on-chip
%bandwidths for an identical code setup.

\section{Residual Computation}
%IS it used to measure convergence in the iterative solver? (i.e. Used
%in a stop criterion?)

Reduction kernel

%Three immediate options
%1) Use an optimized library routine for the reduction.
%2) Doen't do the reduction if it only measures convergence. Choose a fixed
%number of iteration for now, to avoid the reduction step, i.e. Choose an
%adequate number of fixed iteration that will be used in the calculations.
%3) Write your own optimized reduction kernel (cf. Report and source code
%attached from another special course i did with some students last year on
%efficient sum reduction kernels) or just see if you can simply use the
%code attached.



Kernels are executed asynchronously
queuing memory resources asynchronously
using events to track the execution status

problem of kernel load overheads and possibly decrease
necessary loads/stores.

kernels memory bound

%Did you try not to synchronize? Iterative methods may achieve faster
%convergence rate without synchornization. Danger is read/write hazards
%which can penalize performance.
%
%>Probably it would
%>simply have to be a barrier in the end of each iteration.
%Try this
%
%
%Naive kernel means simplest kernel you can think of and without any
%optimizations. Go for correctness first and then optimize and measure
%improvements against this naive kernel. This is one good development
%strategy.


%Reduction kernels
%Yes it is a reduction kernel which requires global communication. This can
%be optized quite far as described in the report by Klaus and Max which I
%gave to you (as I recall).


%>The overall
%>performance of the solver mostly depends on the number of iteration
%>and how many time each kernel is run and what type of kernel is this
%>(whether a simple map or a reduction or stencil). There is an overall
%>speedup when max number of iterations is small (say 20). For 100 that
%>seemed to be a default value in Griebel's code there is a slowdown I
%>wrote about last time. More iterations better detail I understand.,
%>but is there any itermax that would be high enough to be
%>scientifically significant?

%Not sure what you actually see or how you measure performance improvements
%here. I would expect the parallel GPU kernels to be significantly faster
%than the equivalent CPU versions, due to the higher on-chip bandwidth as
%well as the much better latency-hiding when multiple threads are used
%(assuming enough work to do this).


%. Have you profiled the code to spot bottlenecks?
%
%Read and writes are expensive as you pointed out. This comes with the
%modularity of the code chosen.
%
%\subsection{Double/Float Comparison}
%
%>It is also surprising to see divergence in results between single and
%>double-precision floats on CPU and GPU.
%
%How much divergence? Divergence can suggest that there is some difference
%in the algorithms as implemented.