\chapter{Design and Implementation}

\section{Design}
The design of the code is based on the implementation by Griebel et al. that is presented in their book\cite{griebel1998numerical}. Description of most of the functions is featured in the book, so here only the essential changes to functions will be described.

To allow for computations on boundaries, ghost cells need to be allocated.

Numerical methods at most and especially iterative methods map into programming languages in form of loops. As long as consequent iterations are not dependant on one another, programmer can strive for data-parallelism on GPU. Nested loops in numerical methods for 2D problems iterate usually over time and space of the domain. The most common form for a stream to take in GPGPU is a 2D grid because this fits naturally with the rendering model built into GPUs. The base code already simulated the code in 2D. As a side note, the allocated memory in Griebel's code was for verbosity allocated in two-dimensional array. Such abstraction of memory is not supported by OpenCL C, so one-dimensional access had to used.

However, boundary conditions apply only to a small subset of cells in the grid. These are checked using if-conditions that are mapped into branches by compiler. In comparison to GPUs, CPUs have many mechanisms like dynamic branch prediction embedded in the chip. This is because a significant part of the silicon is devoted to control units as opposed to a paradigm of GPU architecture, where most of the space is taken by ALU performing computation. Therefore code that is full of nested conditions, but do not expose much iteration will not expose good performance on GPU. Such code is better left to be executed on CPU.

Necessary changes had to be introduced to allow it to work with GPU. First of all, the OpenCL platform needs to instantiated and the pipeline must be setup.  


A set of kernels was implemented matching every function in Griebel's code. Griebel et al. in their book claim that they aim for modularity of code

Code in project is based on structure from Griebel's code. Least as possible was changed in structure as to keep it comparable with the original code.


Griebel's code is completely sequential. Essentially, all code should be executed on GPU for valid comparison. 

To increase parallelism that can be used on GPU the algorithm needs to be run on single data units as independently as possible.

In terms of way the kernel treats data is considered, there can be different types of kernels. The most simple kernel is the map kernel that maps to every data object a given function and executes it on it. The map operation is straightforward to implement on the GPU as this pure data parallelism. On the GPU performance of such a kernel is limited only be the size of workgroup that is mapped to warps or waves that are in tun mapped to separate sequential multiprocessors or cores. After a fragment of data grid is processes the result is saved in the output buffer of the same size as the input buffer.

Other type of a kernel is a reduction kernel that produce a result that is considerably smaller than the original input. Generally a reduction can be accomplished in multiple steps. For instance for a resultant output of size 1, the results from the prior step are used as the input for the current step and the range over which the operation is applied is reduced until only one stream element remains.

There are also kernels that access the input buffer in random fashion gathering values from any grid cell or a multiple of grid cells at a time.

Another type of kernel is a stencil kernel, which access a fixed number of neighbouring cells and computes its value on their basis.

At high level, application should aasign to each processor the type of work id does best, so serial workloads should stay on host and parallel workloads should be put on device. 

For devices of compute capability 2.x and higher, multiple kernels can execute concurrently on a device, so maximum utilization can also be achieved by using streams to enable enough kernels to execute concurrently. \cite{nvidia2009openclprog}

\subsection{Types of functions}

\section{Implementing OpenCL on NVIDIA platform}
Implementation of OpenCL may vary from platform to platform. Usually the underlying architecture needs to be taken into consideration, when optimizing code for performance. Many techniques are already well known best practices and were proved to yield improvements in performance in many projects before. Therefore, they should be used in this project and analysed what improvements they give in this implementation. Many optimizations and best practices as well as the way the execution and memory model is mapped to device can be common for devices of other vendors. Thus their impact should be always investigated.

\subsection{OpenCL Execution Model}
On NVIDIA platform, OpenCL workitems are executed by CUDA streaming processors (scalar processors or compute elements) and a single workitem is mapped directly to single hardware managed thread. Thread will be blocked if one of the operands is not ready. Reading memory does not block the thread. latency is hidden, because threads are switched, so enough workitem are needed to hide latency. Workgroups are executed on CUDA streaming multiprocessors (compute units) and they do not migrate. However, several concurrent work-groups can reside on one multiprocessor. This is only limited by multiprocessor resources. A whole kernel is launched as an ND Range of workgroups on a sinel CUDA-enabled GPU (compute device). Workgroups are dynamically load-balanced by hardware scheduler. 

\subsection{OpenCL Memory Model on NVIDIA}
On NVIDIA platform, each hardware thread has a dedicated \texttt{\_\_private} region of registers for stack of variables. Each multiprocessor allocates dedicated storage for \texttt{\_\_local} memory and \texttt{\_\_constant} caches. For \texttt{\_\_constant} memory data is stored in global memory. Workitems running on a multiprocessor can communicate through \texttt{\_\_local} shared memory. All workgroups on the device can access \texttt{\_\_global} memory that is stored in global memory available to all multiprocessors on device. Atomic operations allow powerful forms of global communication.

\subsection{OpenCL Synchronization}
On a level of workitem, synchronization is achieved through memory system control using \texttt{mem\_fence()} functions. Independent atomic operations \texttt{atom\_*()} may also be used. For workgroups synchronization, \texttt{barrier()} function is supported as a single instruction fast barrier directly in hardware. Collective operations like \texttt{async\_work\_group\_copy()} leverage the entire multiprocessor. On level of whole ND Range grid, direct hardware support is provided. While using enqueued commands on grid, e.g. \texttt{EnqueueNDRange}, an \texttt{cl\_event} can be used for scheduling which is directly supported in hardware.

\subsection{Best practices}
It is worth mentioning that NVIDIA GPUs have a scalar architecture, so vector types can be rather used for convenience and code readability than for performance as these are not going to be mapped into architecture. In general, on NVIDIA architecture a larger number of workitems gives better performance than wide vectors per workitem. Furthermore, performance can be significantly optimized by overlapping memory with hardware computation. This is why high ratio between arithmetic operations and memory transaction is especially valuable. NVIDIA architecture provides many concurrent workitems to facilitate that. Also asynchronous transfers should be investigated as they permit overlapping computation and bulk transfer.

Moreover, using \texttt{\_\_local} memory should be taken into advantage, as it provides access that is hundred times faster than \texttt{\_\_global} memory. In addition, workitems can cooperate via this type of memory with much lower overhead using \texttt{barrier()} operation with \texttt{CLK\_LOCAL\_MEM\_FENCE}. Next step is to maximize the reuse of shared memory through locality management by staging loads and stores.

Next step is to optimize memory access. Locality of \texttt{\_\_global} memory access patterns should be assessed. For instance, hardware coalescing of access within 128-byte memory blocks yields huge performance improvement effect. Another approach is to use cached texture memory that in OpenCL are access through Image format. Spatial locality of accesses can be optimized as image reads benefit from processing as 2D blocks. In this case experiments with different workgroup sizes should be done. Finally, OpenCL should be allowed to allocate memory optimally. This can be achieved through specifying \texttt{CL\_MEM\_ALLOC\_HOST\_PTR} option while invoking \texttt{clCreateBuffer} for device buffers. Then the implementation can optimize alignment and location. Memory can still be access from the host via \texttt{clEnqueueMap} if needed.

For transfer/compute separate command queues can be used as they can overlap. This yields best results when transfer and compute time is balanced and is most useful when data has high reuse. Another approach is to use Pinned (non-pageable) memory and to directly pass \texttt{ALLOC\_HOST} memory to kernel so that GPUs latency hinding is used to ensure maximal bus usage. This approach gives best performance when there is nearly no reuse of data. Then, no events are needed to synchronize between copy and kernel.

Control flow needs to managed. Different execution paths within workgroup are serialized. Different workgroups can execute different code with no impact on performance. Diverging within a workgroup should be avoided, e.g. branch granularity should be a whole multiple of workgroup size.In general used algorithms should be parallel in nature and should not conist of complex control, data structures or excessive message passing.

In terms of the computation, it should be partitioned to such extent as it keeps all the GPU multiprocessors equally busy. This boils down to keeping a number of workgroups and workitems in them high. In particular, number of workgroups should be much bigger then the number of Compute Units. A good estimate is said to be 256-512 workitems per workgroup. At the same time, the resource usage should be kept low enough to allow multiple active workgroups to be processed by single multiprocessor. This is dependant on the number of registers and \texttt{\_\_local} memory used by workgroup. Am Occupancy Calculator tool provided by NVIDIA might be useful to get a rough estimate.

Finally, optimized math functions should be used where possible and an impact of compiler options should be investigated. In particular, two should be considered \texttt{-cl-mad-enable} that allow using FMADs, computing multiplication-addition operations with less accuracy and \texttt{-cl-fast-relaxed-math} that enables many aggressive compiler optimizations for single- and double-precision floating-point arithmetic that may violate the IEEE 754 standard. These options are passed to \texttt{clBuildProgram} function that compiles and links a program executable from the OpenCL program source or binary.\cite{nvidia2011openclbest, nvidia2012openclprog} As a sidenote, it is a good practice to avoid automatic conversion of doubles to floats by using single-precision floating-point constants, defined with an \texttt{f} suffix \texttt{3.141592653589793f}, \texttt{1.0f}, \texttt{0.5f}.

Shared memory holds the parameters or arguments that are passed to kernels at launch, so in kernels with long argument lists, it can be valuable to put some arguments into constant memory (and reference them there) rather than consume shared memory.

\subsection{Profiling}

\section{Implementing OpenCL on AMD platform}

\section{Implementing OpenCL on Intel platform}

\section{Implementation}
Code is written in C on the host and OpenCL C is used to write kernels to be executed on the device. The overall process of setting up the OpenCL environment will be dropped for this project as there are plenty of resources, from original Khronos materials to tutorial in the Internet, that guide a beginner programmer. Moreover, a specification of OpenCL talks about its constructs, programming and memory model. Therefore only essential constructs that are used in implementation are described in the report. 

The global worksize of computation on GPU is dependant on specified size of the simulated grid. A human-readable definition file is used to set parameters and create the benchmark problem to be simulated by solver. In Griebel's code the parameters are originally specified in inputfile, but the geometry of the boundaries is hardcoded in the source code. This was left unchanged.

Implementation consists of sets of kernels for every of the functions from original code. Modularity was thus maintained, but the effect that it has on overall performance (loading many small kernels vs. a few complex kernels) needs to be determined in tests.

Griebel's code have some intrinsics. For instance, the original code seems to be written for maximal readability and conformance with the content in the book. Thus ghost cells for boundary conditions are additionally allocated.

Code proved to be portable and works under both on Windows and Unix platforms without any additional changes. The code for the project was first and foremost implemented on Windows 7 platform. The eventual execution on Linux platform was pushed to the test part of implementation. Compilers used are nvcc and Visual Studio on Windows. gcc is used on Linux.

\subsection{Benchmarking code}
Code is timed with standard library time.h clock function. The OpenCL profiling timer is not used in this code because the goal is to measure wall clock time on CPU. Code can be timed with buffer allocation and reading it or without. To compare the performance with CPU that accesses its memory on a constant basis, the memory allocation on the device and then later reading it back should be counted in for a fair comparison.

To visualise results of computation, Matlab scripts were written. In original code, Griebel et all. added a possibility to save particle trajectories for visualisation purposes. This was kept, although it is turned off for performance reasons. It was not ported to GPU.

To implement a naive kernel, an easiest approach is to strive for the straightforward unoptimized port of the function. This usually boils down to understanding that kernel is like a (inner nested) loop's body, so the first thing to do is to e.g. getting rid of for loops. In this moment, the boundaries still should not be not crossed so if-conditions should be added to ensure that.

Implementing shared memory kernels usually does not require any changes in the underlying algorithm. Instead, focus is pushed to the host where the kernel is executed and code of each kernel is tuned by finding optimal configuration for work item/ work groups. This is most often done through straightforward timing experiments and brute-force testing.

Another guideline for optimal OpenCL development is that production code should systematically check the error code returned by each API call and check for failures in kernel launches (or groups of kernel launches in the case of concurrent kernels). 

No comparison to other project except a code by Intel\cite{intel2012fluid}