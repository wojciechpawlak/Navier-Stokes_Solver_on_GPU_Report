\chapter{Survey of GPGPU programming}
\section{General trends in GPGPU programming}
General-purpose computing on graphics processing units (abbreviated GPGPU) is a technique that utilizes graphics processing units (GPUs). Initially they were used to handle advanced computations in computer graphics in games and visualisations. However, there is a growing trend to extend their usage to perform data-intensive computations that not long ago were still bound to CPUs only. This means an overall acceleration in variuos general-purpose scientific and engineering applications. In this project, a GPU-based computation-intensive program was implemented, so the platform at hand had to be thoroughly investigated to make most use of it. Thus, this section will focus on this platform mainly and will try to define current efforts in GPGPU programming as of July 2012. The descriptions will be supported with personal experiences gathered during the development process.

Massively parallel, programmable GPU architecture enables superior performance and power efficiency through thousands of smaller, more efficient cores that are mainly intended to be used for parallel computation. Current CPUs consist of a few cores optimized for serial processing. While CPU runs the remainder of the code that mainly controls the flow of the program, GPU allow for order of magnitude faster processing of data through parallelizable data-intensive algorithms. To exploit the high performance of GPUs though, the implementation must express sufficient fine-grained parallelism. Moreover, GPUâ€“CPU data transfer should be minimized and coalesced memory access achieved. A programming model to achieve that has to be provided yielding best performance results possible with a burden of low-level programming. Although the field is relatively young, it progresses extremely fast an has already hit the mainstream.

\figuremacroW{surv_newera}{A new era of processor performance.}{A new era of processor performance\cite{amd2012hipeac}}{1}

First immediate observation is that there is a still diffusion of non standardized methods in the field, although standardization efforts can be observed. OpenCL is the currently dominant open general-purpose GPU computing language. Dominant proprietary framework is NVIDIA's CUDA framework. One option for writing code to execute on GPU is thus to use directly one of these frameworks. However, for a large scientific Fortran or C code, that was written long before the GPUs even existed, this requires a large effort to rewrite, debug, and maintain a separate version of the code.

Furthermore, in the advent of heterogeneous computing systems of which CPU-GPU combination is an example grow in their complexity. They usually consist of a variety of device architectures like: multi-core CPUs, GPUs, Accelerated Processing Units (APUs) or specialized FPGAs to name a a few. Programs need to be designed to work on multiple platforms and be implemented with parallelism in mind that span over number of devices. Such scale is not addressed in this report though.

Currently, there are two significant industry players in the GPU market: NVIDIA and AMD. This is due to the fact that they are two remaining vendors of GPU architectures nowadays. 

As a GPU programming pioneer and 5-years-long-runner NVIDIA has succeeded in providing means to program their proprietary hardware. Most of their efforts go into marketing their CUDA technology and their proprietary platforms that support it. From NVIDIA's perspective the main goal now is to bring GPU computing into the post-CUDA age. CUDA C and Fortran are the most widely used programming languages for GPU programming nowadays. However, as the underlying technology is proprietary to NVIDIA and a software model of GPU computing offered is relatively low-level, the use of CUDA today tends to be restricted to computer scientists. The average programmer can be discouraged by the number of new constructs to learn and the lack of abstraction. Researchers are mostly interested in ad-hoc boost in performance of their algorithms usually written in high-level languages. They seek automation in process of porting their codes to GPU platforms.

Meanwhile, AMD chose another approach and is busy implementing and supporting the latest specification of OpenCL standard. OpenCL is the only technology that can be used with AMD GPU hardware.

Other companies like Intel try to catch up on them introducing their own specialized hardware - CPUs with embedded Hardware Graphics and technologies and providing implementation and support for the current standards like OpenCL.

\section{OpenCL}
OpenCL (Open Computing Language) is an open standard that defined and maintained by Khronos Group and was initially developed by Apple Inc. It is a vendor-independent framework that was from the beginning meant to enable the programmer to develop portable code that can be executed on heterogeneous platforms. The execution and memory model can be mapped to wide range of architectures consisting of CPUs, GPUs and potentially other processing units like APUs or FPGAs. Thanks to that it form a real base in the age of heterogeneous computing. Currently, it has several adopters, among others Intel, AMD, NVIDIA and ARM Holdings.\cite{khronos2012adopt}

OpenCL consists of a programming language based on C99 used to write kernels. These are basic units of executable code, C-like functions that are executed on devices that support OpenCL. It also comprise of a set of APIs. Runtime API is used to control kernel execution and manage the scheduling, computation and the memory resources. The Platform Layer API defines the environment called a context on the platform at hand and creates a hardware abstraction layer over diverse computational resources. Parallel programming in kernels is supported on the level of task-based and data-based parallelism.\cite{khronos2012cloverview}

A compatible compiler like Visual C++ compiler, gcc, Nvidia's nvcc or Intel's compiler is used to compile a collection of kernels and other supporting functions into an OpenCL Program that is executed on the GPU. This is analogous to a dynamic library in C. An application has a queue that stores kernel execution instances in-order. The execution though might be in-order or out-of-order. The most basic unit of work on an OpenCL device is called a work item.

As OpenCL matures, lots of learning materials can be found in the Internet. In addition, there are numerous courses held at universities all over the world that choose OpenCL as a technology for teaching High Performance Scientific Computing. However, a significant sign of the maturity of some technology can be usually measured in the number of the published books available in the market. OpenCL has a few from which a book by Gaster et al. \cite{gaster2011heterogeneous} seemed to be most helpful.

When it comes to the usage of OpenCL in industry, academic institutions, research labs and FPGA vendors are leading implementers. The OpenCL standard homepage\cite{khronos2012opencl} specifies a number of specialized products that use OpenCL. To name some more popular customer products, latest version of Adobe Photoshop CS6 and Premiere CS6\cite{adobe2012} uses OpenCL to accelerate the image operation features including Blur Gallery, Liquify, and Oil Paint. A different usage is a Winzip 16.5 that uses OpenCL to accelerate extraction and compression of the archives with AES encryption. Yet another software that use OpenCL API are GPU-enhanced functions in Wolfram Mathematica.\cite{amd2012apps, amd2012apps2} A tool worth-mentioning is also OpenCL Studio\cite{openclstudio2012}, that integrates OpenCL and OpenGL into a single development environment for high performance computing and visualization. Kishonti Informatics implemented and CLBenchmark\cite{clbenchmark2012} for measuring and comparing the processing power of different hardware architectures that use OpenCL 1.1.

There are different OpenCL Desktop Implementations by NVIDIA\cite{nvidia2012opencl}, AMD\citep{amd2012devzone} and Intel\cite{intel2012openclsdk}. NVIDIA supports OpenCL through its normal drivers and CUDA SDK. CUDA toolchain provides limited support for compilation and profiling OpenCL code. On NVIDIA's CUDA-enabled GPUs OpenCL compiles into PTX ISA. Profiling of the signals and instrumentation is possible.

AMD supports the programmers with its AMD APP SDK to allow for development on their platform. AMD APP (Accelerated Parallel Processing) SDK, formerly ATI Stream, is the OpenCL library for AMD graphics cards. It also provides Math Libraries, gDEBugger, APP Profiler and KernelAnalyzer.

Intel supports OpenCL through its own Intel SDK for OpenCL Applications 2012 released recently. 

\subsection{Current specification}
The current specification of the OpenCL API is 1.2 released on 15 November 2011. However, as each of the adopters implement the standard specification on their own, the scope and support for new version varies. What can be said is that all of them guarantee a support for the version 1.1 of standard. However, some of them like AMD competes to adopt new standard as it introduces some valuable improvements to the OpenCL programming model. In particular, subdevice abstraction provides ways to partition a device into parts that can be treated as separate device could in 1.1. Partitioning of devices gives more control over assignment of computation to compute units. Furthermore, custom devices and built-in kernels address the problem with embedded platforms like specialized FPGAs or non-programmable hardware with associated firmware (e.g. video encoder/decoders or DSPs). These cannot support OpenCL C directly, so built-in kernels can represent these hardware and firmware capabilities. Development closer to GPU touched the Image format of memory resources.\cite{khronos2012cloverview, khronos2012release}

\subsection{Comparison with CUDA}
Comparisons between CUDA and OpenCL are inevitable. Both provide a general-purpose model for data parallelism and low-level access to hardware. Although OpenCL 1.0 was introduced in December 2008, a year and a half after the NVIDIA launched first CUDA, OpenCL still trails CUDA in popularity by a wide margin, especially with regard to HPC and academic world.\cite{hpcwire2012reserachers} However, it is said that if the OpenCL implementation is correctly tweaked to suit the target architecture, it should not perform worse than CUDA. The key feature of OpenCL is its portability as the memory and execution model are abstracted. That limits the optimization of OpenCL code. In CUDA, programmer can directly use GPU-specific technologies, due to the fact it is limited to NVIDIA hardware. Thanks to that CUDA provides more mature compiler optimisations and execution techniques such as more aggressive pragma unroll addition to loops. In OpenCL, programmer is required to add in the optimisations manually. However, as OpenCL toolkit matures differences between these two technologies will vanish.\cite{hpcwire2012openclgains, openclnews2012}

Another point is that there is already a number of libraries supported on CUDA like CUBLAS, CUFFT or CUSPARSE. In comparison, OpenCL still lack high quality, open libraries, although there are recent developments such as:

\begin{itemize}
\item ViennaCL library for linear algebra routines\cite{viennacl2012}
\item GATLAS (GPU Automatically Tuned Linear Algebra Software)\cite{gatlas2012}
\item AMD's clAmdBlas or clAmdFft libraries\cite{amd2012libs}
\item clpp - OpenCL Data Parallel Primitives Library\cite{clpp2012}
\item proprietary AccelerEyes ArrayFire for faster C, C++, Fortran, Python code that uses ArrayFire OpenCL API)
\end{itemize}
 
  that are a symptom that the situation might change in the nearest future. Moreover, ArrayFire made a comparison between two technologies.\cite{accelereyes2012vs} As of now, most research projects seem to be implemented in CUDA. On the other hand, many projects like Par4All\cite{par4all2012} head on in different direction providing the automation of porting the legacy code to GPU technologies. It generates both OpenCL and CUDA code. 

\subsection{WebCL}
WebCL is a set of JavaScript binding to OpenCL that allows for parallel computation in web applications. JavaScript was never designed to exploit the multithreaded data-parallel computing capabilities available on GPUs. On 17 April 2012 Khronos Group released a WebCL working draft and the standard lack any serious implementation or support.\cite{khronos2012webcl}

\subsection{Future developments}
Khronos Group Representatives in its presentations talk among other about OpenCL-HLM, high-level programming model that would introduce new language syntax to unify the host and device execution in order to increase usability and possibilities of optimization. At the same time, OpenCL-SPIR that stands for Standard Parallel Intermediate Representation aims to explore ways to provide target back-end for alternative high-level languages. It also says that Long-Term Core Roadmap is to \enquote{explore enhanced memory and execution model flexibility to catalyse and expose emerging hardware capabilities}.\cite{khronos2012cloverview} Furthermore, it might be true that OpenCL needs a vendor that will boost its standardization, adaptation and marketing process as NVIDIA does for CUDA. AMD has a chance to take this part as they decided to support an open standard OpenCL instead of introducing their own technology. On one hand NVIDIA and Intel provide software handles to OpenCL for their respective hardware. On the other hand, although both Intel and NVIDIA have signed on to OpenCL standard committee and so both technically support it, the performance benefits that they offer are pretty poor when compared to AMD's APUs and GPUs. Another downside to OpenCL, similar to the one with OpenGL, is that everyone in the standardization committee has a say in development of a standard. As a consequence that may lead to conflicts and blocking ideas that are another cause of possible slow progression of OpenCL development in future, although in this moment development of next version of specification is underway.\cite{amd2012hipeac}

\section{Other GPGPU Technologies}
\subsection{CUDA}
The current version of NVIDIA's technology is 4.2. CUDA has a rich support from its vendor and is well adapted through out the industry. It has a highly-optimized libraries and rich toolchain. Checking Compute Capability of NVIDIA cards is the way to measure the advance of CUDA platform. Currently latest version is 3.0, which is implemented in latest Kepler GPUs that went into market earlier this year. \enquote{The Compute Capability of a device is defined by a major revision number and a minor revision number. Devices with the same major revision number are of the same core architecture. The major revision number is 2 for devices based on the Fermi architecture, and 1 for devices based on the Tesla architecture.}\cite{nvidia2012openclprog} In Decemeber last year NVIDIA made its nvcc compiler open-source that might be seen as slight movement towards open standard, which still seems rather probable.\cite{hpcwire2012nvidia} CUDA has bindings to all the most popular programming languages.

\subsection{OpenACC}
The OpenACC API is a collection of compiler directives in fashion of OpenMP.\cite{openacc2012, openacc2012news2} It is a new parallel programming standard for accelerators developed by Cray, CAPS, Nvidia and PGI.  Directives are used to specify loops and regions of code in programming languages like C, C++ or Fortran that are then offloaded by the OpenACC API-enabled compilers and runtimes from the host CPU to the attached accelerator. The resultant code is portable across other accelerators and multicore CPUs. In future, this standard may become a part of the OpenMP standard, as OpenACC was derived from work done within the OpenMP Working Group on Accelerators.

\subsection{C++ AMP}
C++ AMP (Accelerated Massive Parallelism) is meant to accelerate the C++ code by taking advantage of data-parallel hardware like GPUs.\cite{microsoft2012cppamp, microsoft2012cppamp2} It has a set of features like multidimensional arrays, indexing, memory transfer, and tiling supported by a mathematical function library. Transfer between CPU and GPU can be controlled in OpenCL fashion. It is a library implemented on DirectX 11. Microsoft specified an open specification for implementing data parallelism directly in C++. It is targeted to developers with even no expertise in parallization. The code stays portable. Technology is available in latest version of Visual Studio 2012.

\subsection{DirectCompute}
Microsoft DirectCompute is an API that supports GPGPU programming on Windows platform.\cite{microsoft2012direct} It is a compute shader, a programmable shader stage that expands Microsoft Direct3D 11, a part of Microsoft DirectX 11 API, beyond graphics programming. DirectCompute shares a range of computational interfaces  with OpenCL and CUDA. DirectCompute is programmed by a language which is similar to HLSL that is a DirectX shader language.

\subsection{OpenHMPP}
Another example of a set of OpenMP-like directives that preserve legacy codes is OpenHMPP (Hybrid Multicore Parallel Programming), a programming standard for heterogeneous computing.\cite{openhmpp2012, caps2012hmpp} The software is independent from hardware , thus ready for future architectures. The directive-based model is based on works by CAPS (Compiler and Architecture for Embedded and Superscalar Processors) called HMPP. The main concept are the codelets, that are functions that are executed remotely on hardware. HMPP provides synchronous and asynchronous RPC. Like in OpenCL Memory model there are two address spaces: the host processor one and the HWA memory.

\section{Current advances in GPU Architectures}
As mentioned already, today we have two main viable and competitive GPU product lines. NVIDIA and AMD GPUs support a wide range of programming languages. GPUs also make their way to High Performance Computing Centres and supercomputers in academic, research and government labs.\cite{nvidia2012super, nvidia2012super2} As of June 2012, 3 out of 10 Top500 supercomputers in the world were using NVIDIA Tesla Fermi GPUs.\cite{top5002012} In addition, it seems like Tesla Fermi was a significant turning point, when it come to the number of GPU supercomputers in the Top500 list.

In general, heterogeneous system architectures will head in direction of architectural integration unify address space between CPU and GPU to achieve fully coherent memory between them. Physical like in current APUs will mean that CPU and GPUs will be integrated into one chip with a unified memory controller and common manufacturing technology. \cite{amd2012hipeac} GPU will act as a co-processor.

\subsection{NVIDIA}

\subsubsection{Fermi}
GeForce 400 and 500 series cards were the ones, where completely-rebuilt Fermi architecture was introduced.\cite{nvidia2012fermiwhite} \enquote{The GPU featured up to 512 CUDA cores organized in 16 streaming multiprocessors of 32 cores each. It has six 64-bit memory partitions, for a 384-bit memory interface, supporting up to a total of 6 GB of GDDR5 DRAM memory}. With this iteration NVIDIA started to support Visual Studio and C++.

\subsubsection{Tesla}
The main difference between Fermi-based Tesla cards and the GeForce 500 (also Fermi) series is the unlocked double-precision floating-point performance. This is why these cards are targeted to high performance markets, where floating-point calculations on large scale are performed. It is said 1/2 of peak single-precision floating point performance in Tesla cards compared to 1/8 for GeForce cards. In addition, the Tesla cards have ECC-protected memory and are available in models with higher on-board memory (up to 6GB). So this why Tesla is for workloads where data reliability and overall performance are critical.

\subsubsection{Kepler}
Earlier this year NVIDIA has shipped cards with Kepler architecture.\cite{nvidia2012keplerwhite} It also ships a family of TESLA GPUs based on the this architecture. Dynamic Parallelism is the most significant development that allows for regions of computation to be dynamically adjusted. It is based on the idea that the calculations are more fain grained in highly computational regions in excess of more coarse calculations in regions of the grid where there is not much computation involved. Moreover, Cuda 5 parallel programming model is planned to be widely available in the third quarter of 2012 on these GPUs.\cite{kepler2012news1, kepler2012news2}

\subsection{AMD}

\subsubsection{APUs}
APU stands for Accelerated Processing Unit and is a processing system with additional processing capabilities to accelrate given types of computations outsie of a CPU. They may embed a GPGPU, an FPGA or other specialized unit. AMD Fusion is a series of AMD's APUs. What might be interesting from perspective of a GPU programmer is that GPU in such chipset can access host CPU memory without going through a device driver as the memory will have a unified memory controller for both CPU and GPU. Current APUs support OpenCL in version 1.1, although AMD announced on its homepage AMD Accelerated Parallel Processing (APP) SDK with OpenCL 1.2 Support targeting APUs. It can be seen in introduction of Heterogeneous Systems Architecture (HSA) that promises higher facilitation of OpenCL, CPU-GPU seamless cooperation and immediate port to AMD APUs.

\subsubsection{GPUs}
The latest graphic cards by AMD are Northern Islands (HD 6xxx) series and Southern Islands (HD 7xxx) series. Some of the former (HD 63xx, 64xx, and 69xx models) and all the latter support OpenCL's latest 1.2 specification. That is another argument that shows AMD concentrates on supporting OpenCL. The architectures used currently are as follows. The older VLIW5 (5-way VLIW) was used in all models up to HD 6870 in Northern Islands and up to HD76xx in Souther Islands series. More recent VLIW4 (4-way VLIW) architecture was used in HD 69xx models. However, the latest generations of AMD GPUs are already based on new GCN (Graphics Core Next) architecture, where the models HD 77xx-79xx shipped in the beginning of the year are based on it. More detailed overview of all these architectures and the enclosed comparisons may be found in \cite{nielsen2012}. Moreover, the most recent of workstation-oriented GPUs are FirePro cards announced recently. Surprisingly they do not support OpenCL even in 1.1 version. However, AMD in general seems to push its OpenCL strategy for GPU computing.

\subsection{Intel}
Intel, main CPU vendor, is going to release its Many Integrated Core (MIC) architecture for commercial use later this year.\cite{intel2012mic} It is codenamed Knights Corner and the branding name for this processor will be Intel Xeon Phi.\cite{xeon2012} The project incorporates earlier work on Larrabee many core architecture. It will allow for floating point operations on 512-bit SIMD vector registers. Intel wants to leverage x86 legacy and provide architecture that can utilise existing parallelization software tools, among others OpenCL. It is a direct competitor for NVIDIA Tesla in HPC market. Another of Intel's products is released earlier this year 3rd generation Intel Core with Intel HD Graphics, that can be seen as an example of Intel's APU. It can be programmed with OpenCL through recently published Intel SDK for OpenCL Applications 2012.\cite{intel2012openclapp}

\section{Experiences with OpenCL}
\subsection{Developing OpenCL Code}
Most of the development was done on Intel processor and NVIDIA GeForce card on Windows platform. Microsoft Visual Studio 2010 IDE was used for development.

During the development for GPU platform it is invaluable to have an access to a decent debugger that allows the programmer to debug kernels. During this project there was a the lack of debugging options. Such situation was due to the fact that the choice of debugger is dependant on the platform used. Some combinations will then be not supported as of now, when OpenCL is still in its infancy as a standard. So it was the case during the development process for this project as NVIDIA card was combined with OpenCL technology.  Two working options for debugging tried were NVIDIA Nsight (currently in version 4.2) and AMD gDEBugger (currently in version 6.2).\cite{nvidia2012nsight, amd2012gdeb}

The former supports CUDA code only. The tool is an application based on a client-server architecture.  The Visual Studio Plugin serves as a client to the Monitor client process run on the machine. This allows for remote access to GPUs. Moreover, the breakpoints can be set inside the CUDA kernel code so as the current state of local variables, warps and working items will be presented. This valuable data is available through a set of info windows. Tool does not work with OpenCL code though, as it is not possible to set breakpoints in OpenCL code. Tool is distributed as a plugin to Visual Studio 2010.

The later works only with OpenCL code, but supports only the AMD devices. However, it stops on breakpoints on host. Thus it can be partially helpful during the development as it can be configured to stop on specific OpenCL functions and OpenCL errors. It also offers function call history and function properties that gives an overview into the state of parameters that OpenCL API is called with. Finally, there is also an explorer that enables programmer to browse a tree structure of OpenCL constructs used.

On the side, there is an Intel SDK that allows for checking if the code is ready to be run on Intel platform like CPU or new 3rd generation processors with hardware graphics embedded. An offline OpenCL compiler can be used to check the validity of kernel code. 

Development on higher level of abstraction is possible through C++ wrapper API.\cite{khronos2012openclcppspec} However, this was not attempted in this project. Moreover, OpenCL API is ported in form of wrappers to popular programming languages like Java (JavaCL) or Python (PyOpenCL). Such projects surely will advance a wide adoption of OpenCL standard. 

\subsection{Profiling OpenCL Code}
Because most of the development process for this project was done on the NVIDIA platform, some of its tools like NVIDIA Visual Profiler 4.2 or NVIDIA Nsight Profiler Visual Studio Edition 2.2 were researched.\cite{nvidia2012nsight} The latter can only be used with CUDA code, so it was useless for this project. The Visual Profiler was the only program that proved to be helpful as it provided detailed statistics about metrics and events of the application execution. Furthermore, it provides a timeline graph where memory transfers and kernel computations are marked in time. Finally, analysis results provide guidelines that programmer can use to optimize the code. This tool is distributed as part of CUDA Toolkit as of version 4.2. 
Another helpful resource provided by NVIDIA is its CUDA GPU Occupancy Calculator spreadsheet that allow easy calculation of multiprocessor occupancy of a GPU by a given CUDA kernel. This can be used to optimize OpenCL kernels on NVIDIA platform.

Another profiling tools from the competitor AMD are AMD APP Profiler\cite{amd2012appprof} and AMD KernelAnalyzer\cite{amd2012kernelanalyzer}. Gaster et al. in their book\cite{gaster2011heterogeneous} provide an overview and step-by-step description of how to use this tool. These tools were not used during this project, because the code was not developed on the AMD platform. This is an obvious requirement while using these tools. Tools are available for free in the AMD Developer Zone.