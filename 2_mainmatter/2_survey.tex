\chapter{Survey of GPGPU programming}
\section{General trends in GPGPU programming}
General-purpose computing on graphics processing units (abbreviated GPGPU) is a technique that utilizes graphics processing units (GPUs). Initially they were used to handle advanced computations in computer graphics in games and visualisations. However, there is a growing trend to extend their usage to perform data-intensive computations that were until not long ago still bound to CPUs only. This means an overall acceleration in variuos general-purpose scientific and engineering applications. In this project, a GPU-based computation-intensive program was implemented, so the platform at hand had to be thoroughly investigated to make most use of it. Thus, this section will focus on this platform mainly and will try to define current efforts in GPU programming as of July 2012.

Massively parallel, programmable GPU architecture enables superior performance and power efficiency thorugh thousands of smaller, more efficient cores that are mainly intended to be used for parallel computation. Current CPUs consist of a few cores optimized for serial processing. While CPU runs the remainder of the code that mainly controls the flow of the program, GPU allow for order of magnitude faster processing of data through parallelizable data-intensive algorithms. To exploit the high performance of GPUs though, the implementation must express sufficient fine-grained parallelism. Moreover, GPUâ€“CPU data transfer should be minimized and coalesced memory access achieved. A programming model to achieve that has to be provided yielding best performance results possible with a burden of low-level programming. Although the field is relatively young, it progresses extremely fast as already hit the mainstream.

First immediate observation is that there is a still diffusion of non standardized methods in the field, although standardization efforts can be observed. OpenCL is the currently dominant open general-purpose GPU computing language. Dominant proprietary framework is NVIDIA's CUDA framework. One option for writing code to execute on GPU is thus to use directly one of these frameworks. However, for a large scientific Fortran or C code, that was written long before the GPUs even existed, this requires a large effort to rewrite, debug, and maintain a separate version of the code.

Furthermore, in the advent of heterogeneous computing systems of which CPU-GPU combination is an example grow in their complexity. They usually consist of a variety of device architectures like: multi-core CPUs, GPUs, Accelerated Processing Units (APUs) or specialized FPGAs to name a a few. Programs need to be designed to work on multiple platforms and be implemented with parallelism in mind that span over number of devices. Such scale is not addressed in this report though.

Currently, there are two significant industry players in the GPU market: NVIDIA and AMD. This is due to the fact that they are two remaining vendors of GPU architectures nowadays. 

As a GPU programming pioneer and 5-years-long-runner NVIDIA has succeeded in providing means to program their proprietary hardware. Most of their efforts go into marketing their CUDA technology and the platforms that support it. From NVIDIA's perspective the main goal now is to bring GPU computing into the post-CUDA age. CUDA C and Fortran are the most widely used programming languages for GPU programming nowadays. However, as the underlying technology is proprietary to NVIDIA and a software model of GPU computing offered is relatively low-level, the use of CUDA today tends to be restricted to computer scientists. The average programmer can be discouraged by the number of new constructs to learn and the lack of abstraction. Researchers are mostly interested in ad-hoc boost in performance of their algorithms usually written in high-level languages. They seek automation in process of porting their codes to GPU platforms.

Meanwhile, AMD chose another approach and is busy implementing and supporting the latest specification of OpenCL standard.

Other companies like Intel try to catch up on them introducing their own specialized hardware - CPUs with embedded Hardware Graphics and technologies and providing implementation and support for the current standards like OpenCL.



% \hyperref{a}{http://developer.amd.com/sdks/AMDAPPSDK/samples/showcase/Pages/default.aspx}
% \hyperref{b}{http://developer.amd.com/archive/AppShowcaseArchive/Pages/default.aspx}
% \hyperref[c]{http://stackoverflow.com/questions/1126989/what-future-does-the-gpu-have-in-computing}

\section{OpenCL}
OpenCL (Open Computing Language) is an open standard that defined and maintained by Khronos Group and was initially developed by Apple Inc. It is a vendor-independent framework that was from the beginning meant to enable the programmer to develop portable code that can be executed on heterogeneous platforms consisting of CPUs, GPUs and potentially other processing units like APUs or FPGAs. Thanks to that it form a real base in the age of heterogeneous computing. Currently, it has several adopters, among others Intel, AMD, NVIDIA and ARM Holdings.

OpenCL consists of a programming language based on C99 used to write kernels. These are basic units of executable code, C-like functions that are executed on devices that support OpenCL. It also comprise of a set of APIs. Runtime API is used to control kernel execution and manage the scheduling, computation and the memory resources. The Platform Layer API defines the environment called a context on the platform at hand and creates a hardware abstraction layer over diverse computational resources. Parallel programming in kernels is supported on the level of task-based and data-based parallelism.

A compatible compiler like Visual C++ compiler, gcc, Nvidia's nvcc or Intel's compiler is used to compile a collection of kernels and other supporting functions into an OpenCL Program that is executed on the GPU. This is analogous to a dynamic library in C. An application has a queue that stores kernel execution instances in-order. The execution though might be in-order or out-of-order. The most basic unit of work on an OpenCL device is called a work item.

As OpenCL matures, lots of learning materials can be found in the Internet. In addition, there are numerous courses held at universities all over the world that choose OpenCL as a technology for teaching High Performance Scientific Computing. However, a significant sign of the maturity of some technology can be usually measured in the number of the published books available in the market. OpenCL has a few from which a book by Gaster et al. \cite{gaster2011heterogeneous} seemed to be most helpful.

When it comes to the usage of OpenCL in industry, academic institutions, research labs and FPGA vendors are leading implementers. The OpenCL standard homepage\cite{khronos_opencl} specifies a number of specialized products that use OpenCL. To name some more popular customer products, latest version of Adobe Photoshop CS6 uses OpenCL to accelerate the image drawing. A different usage is a Winzip 16.5 that uses OpenCL to accelerate extraction and encryption of the archives. An interesting use of OpenCL is an OpenCL Studio.



% \hyperref{c}{http://www.khronos.org/conformance/adopters/adopter-companies}
% \hyperref{d}{http://www.khronos.org/conformance/adopters/conformant-companies}

%http://developer.amd.com/zones/OpenCLZone/
%http://software.intel.com/en-us/articles/opencl-sdk/
%http://developer.nvidia.com/opencl

% link to OpenCL overview

% links to books

% links to Photoshop, Winzip

% \hyperref[OpenCLStudio]{http://www.youtube.com/user/OpenCLStudio}

\subsection{Current specification}
The current specification of the OpenCL API is 1.2 released November 15, 2011. However, as each of the adopters implement the standard specification on their own, the scope and support for new version varies. What can be said is that all of them guarantee a support for the version 1.1 of standard. However, some of them like AMD competes to adopt new standard as it introduces some valaubale improvements to the OpenCL programming model. In particular, subdevice abstraction provides ways to partition a device into parts that can be treated as seperate device could in 1.1. Partitioning of devices gives more control over assignment of computation to compute units. Furthermore, custom devices and built-in kernels address the problem with embedded platforms like specialized FPGAs or non-programmable hardware with associated firmware (e.g. video encoder/decoders or DSPs). These cannot support OpenCL C directly, so built-in kernels can represent these hardware and firmware capabilities. Development closer to GPU touched the Image format of memory resources.


% overview.pdf
% http://www.khronos.org/news/press/khronos-releases-opencl-1.2-specification

\subsection{Comparison with CUDA}


Multiple comparisons have been drawn between CUDA and OpenCL since its inception.[59][60] They both draw the same conclusions: if the OpenCL implementation is correctly tweaked to suit the target architecture, it performs no worse than CUDA. Because the key feature of OpenCL is portability (via its abstracted memory and execution model), the programmer is not able to directly use GPU-specific technologies, unlike CUDA. CUDA is more acutely aware of the platform upon which it will be executing because it is limited to Nvidia hardware, and therefore, it provides more mature compiler optimisations and execution techniques. Furthermore, the CUDA compiler displayed more mature compilation techniques, such as more aggressive pragma unroll addition to loops. Therefore, the developer is required to add in the optimisations manually to the kernel code. This is indicative of the maturity of the CUDA toolkit versus the newer OpenCL toolkits. It is likely in the future that this gap will be closed as the toolchains mature.

% \hyperref{}{http://www.hpcwire.com/hpcwire/2012-02-28/opencl_gains_ground_on_cuda.html}

\subsection{WebCL}
Bringing parallel computation to the Web through JavaScript binding to OpenCL
On 17 April 2012 Khronos released a WebCL working draft[

\subsection{Future developments}
OpenCL-HLM

OpenCL-SPIR

Long-Term Core Roadmap

%From opencl-overview.pdf last slide

\section{Other GPGPU Technologies}
\subsection{CUDA}

4.2

In Decemeber, NVIDIA made its nvcc compiler open.
% \hyperref{e}{http://www.hpcwire.com/hpcwire/2012-01-26/nvidia_releases_upgraded_cuda_compiler,_visual_profiler,_and_npp_library.html}

\subsection{DirectCompute}
enable cross-platform development

\subsection{C++ AMP (C++ Accelerated Massive Parallelism)}

% \hyperref[C++ AMP MSDN}{http://msdn.microsoft.com/en-us/library/hh265137}

\subsection{OpenACC}
Directives like OpenMP for multicore CPU programming

announcement of a new directives-based parallel programming standard for accelerators.  Called OpenACC, the open standard is intended to bring GPU computing into the realm of the average programmer, while making the resulting code portable across other accelerators and even multicore CPUs
% \hyperref[OpenACC]{http://openacc.org/}
% \hyperref{OpenACC}{http://www.hpcwire.com/hpcwire/2011-12-07/nvidia_eyes_post-cuda_era_of_gpu_computing.html}
% \hyperref{f}{http://www.hpcwire.com/hpcwire/2012-06-20/openacc_group_reports_expanding_support_for_accelerator_programming_standard.html}

But the real end game for OpenACC supporters is for the directives to be incorporated into the OpenMP standard.  Since OpenACC was derived from work done within the OpenMP Working Group on Accelerators,

\subsection{OpemHMPP}

\subsection{AMD Accelerated Parallel Processing (APP) SDK}
formerly ATI Stream

% \hyperref[AMD 1]{http://developer.amd.com/sdks/AMDAPPSDK/Pages/default.aspx}
% \hyperref[AMD 2]{http://www.amd.com/us/products/technologies/amd-app/Pages/eyespeed.aspx}




\section{Current advances in GPU Architectures}
today we have two viable and
competitive product lines, Nvidia and Advanced
Micro Devices (AMD) GPUs, with support for a
wide range of programming languages.

GPUs on super-computers

Clusters of GPUs - HPC 500 - most fast are using the GPUs.

\subsection{NVIDIA}
\subsubsection{Tesla}
\subsubsection{Fermi}
\subsubsection{Kepler}

\subsubsection{Customer Cards}
Geforce, Quadro
Tesla for workloads where data reliability and overall performance are critical 

\subsection{AMD}
\subsubsection{APUs}
\subsubsection{GPUs}
multicore x86

Radeon

AMD APP Acceleration 


AMD is still pushing its OpenCL strategy for GPU computing. 

\subsection{Intel}
\subsubsection{Intel SDK for OpenCL Applications 2012}

Intel's upcoming Many Integrated Core (MIC) coprocessor, Intel MIC





\section{Developing OpenCL Code}
Most of the development was done on Intel Processor and NVIDIA Geforce Card on Windows Platform. Microsoft Visual Studio 2010 IDE was used for development.

The largest problem during GPU programming is lack of debugging options. The choice of debugger is dependant on the platform used. Some combinations will then be not supported. It was the case during the development process for this project as NVIDIA card was combined with OpenCL technology.  Two working options for debugging found: NVIDIA Nsight (currently in version 4.2) and AMD gDEBugger (currently in version 6.2).

First supports CUDA code only. The tool is an application in a client-server architecture.  The Visual Studio Plugin serves as a client to the Monitor client process run on the machine. This allows for remote access to GPUs. Moreover, the breakpoints can be set inside the CUDA kernel code and the current state of local variables, warps and working items will be presented. This valuable data is available through a set of info windows. Warps can be observed. Tool does not work with OpenCL code though. It is not possible to set breakpoints in OpenCL code. Tool is distributed as a plugin to Visual Studio 2010.

The second works only with OpenCL code on the AMD devices. Stops on breakpoints on host. Can be asked to stop on specific OpenCL functions. Offers function call history an function properties. There is also an explorer where OpenCL constructs are presented in a tree structure.

In addition, there is an Intel SDK that allows for checking if the code is ready to be run on Intel platform like CPU or new 3rd generation processors with hardware graphics embedded. An offline compiler is used tor these purposes. 

OpenCL Desktop Implementations


OpenCL Books


Development on higher level of abstraction is possible through C++ wrapper API. This was not tried in this project.

There are ports in form of wrappers to popular programming languages like Java (JavaCL) or Python (PyOpenCL).

\section{Profiling OpenCL Code}
Because most of the development process for this project was done on the NVIDIA platform, some of its tools like NVIDIA Visual Profiler 4.2 or NVIDIA Nsight Profiler Visual Studio Edition 2.2 were researched. The latter can only be used with CUDA code, so it was useless for this project. The Visual Profiler was the only program that proved to be helpful as it provided detailed statistics about metrics and events of the application execution. Furthermore, it provides a timeline graph where memory transfers and kernel computations are marked in time. Finally, analysis results provide guidelines that programmer can use to optimize the code. This tool is distributed as part of CUDA Toolkit as of version 4.2. 
Another helpful resource provided by NVIDIA is its CUDA GPU Occupancy Calculator Spreadsheet that allow easy calculation of multiprocessor occupancy of a GPU by a given CUDA kernel. This can be used to optimize OpenCL kernels on NVIDIA platform.

Another profiling tools from the competitor AMD are AMD APP Profiler and AMD KernelAnalyzer. Gaster et al. in their book\cite{gaster2011heterogeneous} provide an overview and step-by-step description of how to use this tool. These tools were not used during this project, because the code was not developed on the AMD platform. This is an obvious requirement while using these tools. Tools are available for free in the AMD Developer Zone.

